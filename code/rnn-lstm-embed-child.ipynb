{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport os\nimport shutil\nfrom sklearn.model_selection import train_test_split\nimport numpy as np","metadata":{"id":"iq_hk6BqBrN9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_imdb():\n  # download dataset\n  url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n\n  dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n                                    untar=True, cache_dir='.',\n                                    cache_subdir='')\n\n  dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n  train_dir = os.path.join(dataset_dir, 'train')\n  test_dir = os.path.join(dataset_dir, 'test')\n  \n  # remove irrelevant data\n  remove_dir = os.path.join(train_dir, 'unsup')\n  shutil.rmtree(remove_dir)\n\n  # load to dataframes\n  train_lst, test_lst = [], []\n  label2id = {\"pos\" : 1, \"neg\" : 0}\n\n  for label in ['pos', 'neg']:\n    path = train_dir + \"/\" + label\n    files = os.listdir(path)\n    for _file in files:\n      with open(os.path.join(path, _file), 'r') as f:\n        # strip <br /> tags\n        text = f.read()\n        train_lst.append([text, label2id[label]])\n    \n    path = test_dir + \"/\" + label\n    files = os.listdir(path)\n    for _file in files:\n      with open(os.path.join(path, _file), 'r') as f:\n        text = f.read()\n        test_lst.append([text, label2id[label]])\n    \n  df_train = pd.DataFrame(train_lst, columns=['text', 'label']).sample(frac=1)\n  df_test  = pd.DataFrame(test_lst, columns=['text', 'label'])\n  x_train, y_train = df_train[\"text\"], df_train[\"label\"]\n  x_test, y_test = df_test[\"text\"], df_test[\"label\"]\n\n  return x_train, y_train, x_test, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_fin():\n    # download dataset\n    url = '/kaggle/input/financial-sentiment-analysis/data.csv'\n\n    # load to dataframes\n    df_raw = pd.read_csv(url)\n    label2id = {\"positive\" : 2, \"neutral\" : 1, \"negative\" : 0}\n    df_raw[\"Sentiment\"] = df_raw[\"Sentiment\"].apply(lambda x : label2id[x])\n\n    df_train, df_test = train_test_split(df_raw)\n    df_train = df_train.sample(frac=1)\n    \n    x_train, y_train = df_train[\"Sentence\"], df_train[\"Sentiment\"]\n    x_test, y_test = df_test[\"Sentence\"], df_test[\"Sentiment\"]\n\n    return x_train, y_train, x_test, y_test","metadata":{"id":"06r2fOp47fqP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_sst5():\n    train_url = 'https://raw.githubusercontent.com/christycty/sentiment-analysis-review/main/data/sst5_train.csv'\n    test_url = 'https://raw.githubusercontent.com/christycty/sentiment-analysis-review/main/data/sst5_test.csv'\n    \n    df_train = pd.read_csv(train_url).sample(frac=1)\n    df_test = pd.read_csv(test_url)\n    \n    x_train, y_train = df_train[\"sentence\"], df_train[\"label\"]\n    x_test, y_test = df_test[\"sentence\"], df_test[\"label\"]\n\n    return x_train, y_train, x_test, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_sst2():\n    train_url = 'https://raw.githubusercontent.com/christycty/sentiment-analysis-review/main/data/sst5_train.csv'\n    test_url = 'https://raw.githubusercontent.com/christycty/sentiment-analysis-review/main/data/sst5_test.csv'\n    \n    df_train = pd.read_csv(train_url).sample(frac=1)\n    df_test = pd.read_csv(test_url)\n    \n    # remove neutral\n    df_train = df_train[df_train[\"label\"] != 2]\n    df_test = df_test[df_test[\"label\"] != 2]\n    \n    # map to positive or negative\n    label2id = {0:0, 1:0, 3:1, 4:1}\n    df_train[\"label\"] = df_train[\"label\"].apply(lambda x : label2id[x])\n    df_test[\"label\"] = df_test[\"label\"].apply(lambda x : label2id[x])\n    \n    x_train, y_train = df_train[\"sentence\"], df_train[\"label\"]\n    x_test, y_test = df_test[\"sentence\"], df_test[\"label\"]\n\n    return x_train, y_train, x_test, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport re\n\ndef preprocess(text_inp):\n    TAG_RE = re.compile(r'<[^>]+>')\n\n    text = TAG_RE.sub('', text_inp)\n    text = re.sub('[^a-zA-Z]', ' ', text) # non alphabets\n    text = re.sub(r'\\s+', ' ', text)  # multiple space\n    \n    # stopwords\n    text = text.lower().split()\n    stopwords_set = set(stopwords.words('english'))\n    text = [x for x in text if x not in stopwords_set]\n    return \" \".join(text)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlLfv52tX0r4","outputId":"8760b49e-4674-401e-eee1-3f376fc21c1e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_max_len(text1_, text2_, name):\n    text1 = text1_.apply(preprocess)\n    lengths = text1.apply(lambda x:len(str(x).split(' ')))\n    maxlen1 = lengths.max()\n    meanlen1 = lengths.mean()\n    \n    text2 = text2_.apply(preprocess)\n    lengths = text2.apply(lambda x:len(str(x).split(' ')))\n    maxlen2 = lengths.max()\n    meanlen2 = lengths.mean()\n    \n    print(f\"{name}: train max len {maxlen1}, test max len {maxlen2}\")\n    print(f\"{name}: train mean len {meanlen1}, test mean len {meanlen2}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_data():\n    x_train_raw, y_train_, x_test_raw, y_test = load_fin()\n    get_max_len(x_train_raw, x_test_raw, \"fin\")\n    \n    x_train_raw, y_train_, x_test_raw, y_test = load_imdb()\n    get_max_len(x_train_raw, x_test_raw, \"imdb\")\n    \n    x_train_raw, y_train_, x_test_raw, y_test = load_sst5()\n    get_max_len(x_train_raw, x_test_raw, \"sst5\")\n    \n    x_train_raw, y_train_, x_test_raw, y_test = load_sst2()\n    get_max_len(x_train_raw, x_test_raw, \"sst5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\nimport gensim.downloader as api\nimport gensim\n\ndef get_embeddings(dataset, name=\"word2vec\"):\n    # tokenize on training dataset\n    tokenizer = Tokenizer(num_words = 15000, oov_token=\"<oov>\")\n    tokenizer.fit_on_texts(dataset)\n    word_index = tokenizer.word_index\n    print(f\"there are {len(word_index)} unique words in dataset\")\n    # number of words to keep in dictionary\n    num_words = min(15000, len(word_index) + 1)\n    \n    # import the word embeddings\n    if name == \"word2vec\":\n        word2vec_path = '../input/google-word2vec/GoogleNews-vectors-negative300.bin'\n        embeddings = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary = True)\n#         embeddings = api.load('word2vec-google-news-300')\n        embed_dim = 300\n    \n    elif name == \"glove100\":\n        embeddings = {}\n        f = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings[word] = coefs\n        f.close()\n        embed_dim = 100\n    \n    elif name == \"glove200\":\n        embeddings = {}\n        f = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt')\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings[word] = coefs\n        f.close()\n        embed_dim = 200\n    \n    # create embedding matrix (map tokenizer index to word embeddings)\n    embedding_matrix = np.zeros((num_words, embed_dim))\n    \n    for word, i in word_index.items():\n        if i >= num_words:\n            break\n        if word in embeddings:\n            embedding_matrix[i] = embeddings[word]\n        else:\n            embedding_matrix[i] = np.random.randn(embed_dim)\n        \n    return tokenizer, embedding_matrix, embed_dim","metadata":{"id":"Ql3flAeFL9ld","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Part","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers","metadata":{"id":"QtmJSEjFB8hM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_rnn(num_class, num_words, embed_dim, embed_matrix):\n    model = Sequential([\n        tf.keras.layers.Embedding(num_words, \n                                  embed_dim, \n                                  weights=[embed_matrix], \n                                  trainable=True),\n        tf.keras.layers.SimpleRNN(32, dropout=0.5),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n    ])\n    if (num_class == 2):\n        net = tf.keras.layers.Dense(1, activation='sigmoid')\n    else:\n        net = tf.keras.layers.Dense(num_class, activation='softmax')\n    model.add(net)\n    \n    if num_class == 2:\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    else:\n        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","metadata":{"id":"pAJ_iDP-FTKN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_lstm(num_class, num_words, embed_dim, embed_matrix):\n    model = Sequential([\n        tf.keras.layers.Embedding(num_words, \n                                  embed_dim, \n                                  weights=[embed_matrix], \n                                  trainable=True),\n        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, dropout=0.5)),\n        tf.keras.layers.Dense(16, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n    ])\n    \n    if (num_class == 2):\n        net = tf.keras.layers.Dense(1, activation='sigmoid')\n    else:\n        net = tf.keras.layers.Dense(num_class, activation='softmax')\n    model.add(net)\n    \n    if num_class == 2:\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    else:\n        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n  ","metadata":{"id":"WM_PspVRCFWW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_nn(num_class, num_words, embed_dim, embed_matrix):\n    model = Sequential([\n        tf.keras.layers.Embedding(num_words, \n                                  embed_dim, \n                                  weights=[embed_matrix], \n                                  trainable=False),\n        tf.keras.layers.Dense(2048, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(1024, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(256, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(16, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n    ])\n    \n    if (num_class == 2):\n        net = tf.keras.layers.Dense(1, activation='sigmoid')\n    else:\n        net = tf.keras.layers.Dense(num_class, activation='softmax')\n    model.add(net)\n    \n    if num_class == 2:\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    else:\n        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, x_train, y_train, x_val, y_val, epochs=10):\n  # Train the model\n  history = model.fit(x_train, y_train, epochs=epochs,\n                      validation_data=(x_val, y_val))\n\n  # Evaluate the model on the validation set\n  loss, accuracy = model.evaluate(x_val, y_val)\n  print(f'Validation loss: {loss:.4f}, Validation accuracy: {accuracy:.4f}')\n  return history","metadata":{"id":"yiGYB_FvCtG7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model(data, model_name, embed_name, in_epoch=-1):\n    if data == \"fin\":\n        x_train_raw, y_train, x_test_raw, y_test = load_fin()\n        num_classes = 3\n        seq_len = 40  # trim extra\n        \n    elif data == \"imdb\":\n        x_train_raw, y_train, x_test_raw, y_test = load_imdb()\n        num_classes = 2\n        seq_len = 500 # trim extra\n        \n    elif data == \"sst5\":\n        x_train_raw, y_train, x_test_raw, y_test = load_sst5()\n        num_classes = 5\n        seq_len = 30\n        \n    elif data == \"sst2\":\n        x_train_raw, y_train, x_test_raw, y_test = load_sst2()\n        num_classes = 2\n        seq_len = 30\n    \n    # text preprocessing\n    x_train_ = x_train_raw.apply(preprocess)\n    x_test_ = x_test_raw.apply(preprocess)\n    \n    tokenizer, embed_matrix, embed_dim = get_embeddings(x_train_, embed_name)\n    num_words = min(15000, len(tokenizer.word_index) + 1)\n    \n    # tokenize sentences (text to sequence of indices)\n    x_train = tokenizer.texts_to_sequences(x_train_)\n    x_test = tokenizer.texts_to_sequences(x_test_)\n    \n    x_train = pad_sequences(x_train, seq_len, truncating=\"post\")\n    x_test = pad_sequences(x_test, seq_len, truncating=\"post\")\n    \n    # build models\n    if model_name == \"rnn\":\n        model = build_rnn(num_classes, num_words, embed_dim, embed_matrix)\n        epochs = 10\n        \n    elif model_name == \"lstm\":\n        model = build_lstm(num_classes, num_words, embed_dim, embed_matrix)\n        epochs = 8\n        \n    elif model_name == \"nn\":\n        model = build_nn(num_classes, num_words, embed_dim, embed_matrix)\n        epochs = 20\n    \n    if in_epoch != -1:\n        epochs = in_epoch\n        \n    print(model.summary())\n    hist = train(model, x_train, y_train, x_test, y_test, epochs=epochs)\n    \n    model.evaluate(x_test, y_test)\n    model_save = f\"/kaggle/working/{data}_{model_name}\"\n    \n#     model.save(model_save)\n#     shutil.make_archive(model_save, 'zip', \"/kaggle/working\")\n    \n    hist_df = pd.DataFrame(hist.history)\n    hist_df.to_csv(model_save + \"_hist.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model(\"imdb\", \"rnn\", \"word2vec\", 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_model(\"fin\", \"lstm\", \"glove100\", 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_model(\"imdb\", \"lstm\", \"glove100\", 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_model(\"fin\", \"rnn\", \"glove100\", 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_model(\"imdb\", \"rnn\", \"glove100\", 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}