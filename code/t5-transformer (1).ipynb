{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"4685704a-bb0f-470d-b9ea-b0a362c106fc","_cell_guid":"27ae713b-e9b2-4daa-9cd3-b1d240a5a3c5","collapsed":false,"id":"b-P1ZOA0FkVJ","execution":{"iopub.status.busy":"2023-04-21T18:30:39.212710Z","iopub.execute_input":"2023-04-21T18:30:39.213213Z","iopub.status.idle":"2023-04-21T18:30:51.558557Z","shell.execute_reply.started":"2023-04-21T18:30:39.213166Z","shell.execute_reply":"2023-04-21T18:30:51.557263Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport os\nimport json\nimport torch\nimport shutil\n\nimport tensorflow as tf\nimport random\nimport numpy as np\nimport pandas as pd\nfrom transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, create_optimizer\nfrom datasets import load_dataset, concatenate_datasets, Dataset\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"912a7eb1-aeaf-4b4c-b72c-d23faa7180d2","_cell_guid":"1f0db770-5afd-4cb2-8fee-70517206404d","collapsed":false,"id":"_XgTpm9ZxoN9","outputId":"b13341a4-ba06-429e-cdcf-3de5f9d32c35","execution":{"iopub.status.busy":"2023-04-21T19:17:58.705025Z","iopub.execute_input":"2023-04-21T19:17:58.705719Z","iopub.status.idle":"2023-04-21T19:17:58.711786Z","shell.execute_reply.started":"2023-04-21T19:17:58.705681Z","shell.execute_reply":"2023-04-21T19:17:58.710561Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_imdb():\n  # download dataset\n  url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n\n  dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n                                    untar=True, cache_dir='.',\n                                    cache_subdir='')\n\n  dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n  train_dir = os.path.join(dataset_dir, 'train')\n  test_dir = os.path.join(dataset_dir, 'test')\n  \n  # remove irrelevant data\n  remove_dir = os.path.join(train_dir, 'unsup')\n  shutil.rmtree(remove_dir)\n\n  # load to dataframes\n  train_lst, test_lst = [], []\n  label2id = {\"pos\" : \"positive\", \"neg\" : \"negative\"}\n\n  for label in ['pos', 'neg']:\n    path = train_dir + \"/\" + label\n    files = os.listdir(path)\n    for _file in files:\n      with open(os.path.join(path, _file), 'r') as f:\n        # strip <br /> tags\n        text = f.read()\n        train_lst.append([text, label])\n    \n    path = test_dir + \"/\" + label\n    files = os.listdir(path)\n    for _file in files:\n      with open(os.path.join(path, _file), 'r') as f:\n        text = f.read()\n        test_lst.append([text, label2id[label]])\n    \n  df_train = pd.DataFrame(train_lst, columns=['text', 'label']).sample(frac=1)\n  df_test  = pd.DataFrame(test_lst, columns=['text', 'label'])\n  x_train, y_train = df_train[\"text\"], df_train[\"label\"]\n  x_test, y_test = df_test[\"text\"], df_test[\"label\"]\n\n  return x_train, y_train, x_test, y_test","metadata":{"_uuid":"642f46e4-b3d2-47ce-9258-cb69aa2114f5","_cell_guid":"36beb528-182a-470c-87a7-2e47076fd472","collapsed":false,"id":"pOdqCMoQDRJL","execution":{"iopub.status.busy":"2023-04-21T19:34:03.119013Z","iopub.execute_input":"2023-04-21T19:34:03.120129Z","iopub.status.idle":"2023-04-21T19:34:03.131936Z","shell.execute_reply.started":"2023-04-21T19:34:03.120087Z","shell.execute_reply":"2023-04-21T19:34:03.130808Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_fin():\n    # download dataset\n    url = '/kaggle/input/financial-sentiment-analysis/data.csv'\n\n    # load to dataframes\n    df_raw = pd.read_csv(url)\n    label2id = {\"positive\" : 2, \"neutral\" : 1, \"negative\" : 0}\n    # df_raw[\"Sentiment\"] = df_raw[\"Sentiment\"].apply(lambda x : label2id[x])\n\n    df_train, df_test = train_test_split(df_raw)\n    df_train = df_train.sample(frac=1)\n    x_train, y_train = df_train[\"Sentence\"], df_train[\"Sentiment\"]\n    x_test, y_test = df_test[\"Sentence\"], df_test[\"Sentiment\"]\n\n    return x_train, y_train, x_test, y_test","metadata":{"_uuid":"56739e09-2c14-4fdd-8170-a273a987649d","_cell_guid":"b18c9f03-203f-4cd2-b570-bc5fe19cea88","collapsed":false,"id":"6IwI_2bcIeX8","execution":{"iopub.status.busy":"2023-04-21T18:31:01.308512Z","iopub.execute_input":"2023-04-21T18:31:01.309286Z","iopub.status.idle":"2023-04-21T18:31:01.321896Z","shell.execute_reply.started":"2023-04-21T18:31:01.309241Z","shell.execute_reply":"2023-04-21T18:31:01.320664Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_sst5():\n    train_url = 'https://raw.githubusercontent.com/christycty/sentiment-analysis-review/main/data/sst5_train.csv'\n    test_url = 'https://raw.githubusercontent.com/christycty/sentiment-analysis-review/main/data/sst5_test.csv'\n    \n    df_train = pd.read_csv(train_url).sample(frac=1)\n    df_test = pd.read_csv(test_url)\n    \n    id2label = {0:\"very negative\", 1:\"negative\", 2:\"neutral\", 3:\"positive\", 4:\"very positive\"}\n    df_train['label'] = df_train['label'].apply(lambda x : id2label[x])\n    df_test['label'] = df_test['label'].apply(lambda x : id2label[x])\n    \n    x_train, y_train = df_train[\"sentence\"], df_train[\"label\"]\n    x_test, y_test = df_test[\"sentence\"], df_test[\"label\"]\n\n    return x_train, y_train, x_test, y_test","metadata":{"_uuid":"42acd0c6-c80c-4e6a-8e5d-80c918a034b8","_cell_guid":"0302ecea-4179-4748-a5fd-7e739a57a538","collapsed":false,"execution":{"iopub.status.busy":"2023-04-21T18:31:01.326263Z","iopub.execute_input":"2023-04-21T18:31:01.326669Z","iopub.status.idle":"2023-04-21T18:31:01.337352Z","shell.execute_reply.started":"2023-04-21T18:31:01.326634Z","shell.execute_reply":"2023-04-21T18:31:01.336139Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_sst2():\n    train_url = 'https://raw.githubusercontent.com/christycty/sentiment-analysis-review/main/data/sst5_train.csv'\n    test_url = 'https://raw.githubusercontent.com/christycty/sentiment-analysis-review/main/data/sst5_test.csv'\n    \n    df_train = pd.read_csv(train_url).sample(frac=1)\n    df_test = pd.read_csv(test_url)\n    \n    # remove neutral\n    df_train = df_train[df_train[\"label\"] != 2]\n    df_test = df_test[df_test[\"label\"] != 2]\n    \n    # map to positive or negative\n    label2id = {0:\"negative\", 1:\"negative\", 3:\"positive\", 4:\"positive\"}\n    df_train[\"label\"] = df_train[\"label\"].apply(lambda x : label2id[x])\n    df_test[\"label\"] = df_test[\"label\"].apply(lambda x : label2id[x])\n    \n    x_train, y_train = df_train[\"sentence\"], df_train[\"label\"]\n    x_test, y_test = df_test[\"sentence\"], df_test[\"label\"]\n\n    return x_train, y_train, x_test, y_test","metadata":{"_uuid":"08fdb42a-6ade-4446-9170-9632326f9391","_cell_guid":"773aacfe-9b89-41fc-8e4f-0cca522ab37c","collapsed":false,"execution":{"iopub.status.busy":"2023-04-21T18:31:01.339687Z","iopub.execute_input":"2023-04-21T18:31:01.340156Z","iopub.status.idle":"2023-04-21T18:31:01.350154Z","shell.execute_reply.started":"2023-04-21T18:31:01.340114Z","shell.execute_reply":"2023-04-21T18:31:01.348872Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport re\n\ndef preprocess(text_inp, max_words=250):\n    TAG_RE = re.compile(r'<[^>]+>')\n\n    text = TAG_RE.sub('', text_inp)\n    text = re.sub('[^a-zA-Z]', ' ', text) # non alphabets\n    text = re.sub(r'\\s+', ' ', text)  # multiple space\n    \n    # stopwords\n    text = text.lower().split()\n    stopwords_set = set(stopwords.words('english'))\n    text = [x for x in text if x not in stopwords_set]\n    # keep 250 words at most\n    text = text[:min(len(text), max_words)]\n    text = \" \".join(text) + \"</s>\"\n    return text","metadata":{"_uuid":"8bbaa2ed-38c6-4c1a-8c29-d661de69135b","_cell_guid":"327a2b68-8f5d-4fed-afe2-cb8106f3f90e","collapsed":false,"id":"JuxDkcvVIoev","outputId":"f22a5a1b-16bc-4c63-a70f-d8bb794da924","execution":{"iopub.status.busy":"2023-04-21T19:46:00.022854Z","iopub.status.idle":"2023-04-21T19:46:00.024048Z","shell.execute_reply.started":"2023-04-21T19:46:00.023765Z","shell.execute_reply":"2023-04-21T19:46:00.023794Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, x_train, y_train, x_val, y_val, epochs=10):\n  # Train the model\n  history = model.fit(x_train, y_train, epochs=epochs,\n                      validation_data=(x_val, y_val))\n\n  # Evaluate the model on the validation set\n  loss, accuracy = model.evaluate(x_val, y_val)\n  print(f'Validation loss: {loss:.4f}, Validation accuracy: {accuracy:.4f}')\n  return history","metadata":{"_uuid":"645abc2f-ae72-4f16-bf2d-6631c13ba264","_cell_guid":"d1d93ee1-22a8-44d3-9bc4-2f8469fb1dfd","collapsed":false,"id":"tXxYpK8ixL34","execution":{"iopub.status.busy":"2023-04-21T18:31:01.795080Z","iopub.execute_input":"2023-04-21T18:31:01.795717Z","iopub.status.idle":"2023-04-21T18:31:01.802536Z","shell.execute_reply.started":"2023-04-21T18:31:01.795676Z","shell.execute_reply":"2023-04-21T18:31:01.801393Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(data, checkpoint):\n    if data == \"fin\":\n        x_train_raw, y_train_, x_test_raw, y_test = load_fin()\n        num_classes = 3\n    elif data == \"imdb\":\n        x_train_raw, y_train_, x_test_raw, y_test = load_imdb()\n        num_classes = 2\n        steps_per_epoch = 625\n        \n    elif data == \"sst5\":\n        x_train_raw, y_train_, x_test_raw, y_test = load_sst5()\n        num_classes = 5\n        \n    elif data == \"sst2\":\n        x_train_raw, y_train_, x_test_raw, y_test = load_sst2()\n        num_classes = 2\n        steps_per_epoch = 173\n    \n    # preprocess dataset\n    x_train = x_train_raw.apply(preprocess)\n    x_test = x_test_raw.apply(preprocess)\n    train_df = pd.concat([x_train, y_train_], axis=1, keys=['sentence', 'label'])\n    test_df = pd.concat([x_test, y_test], axis=1, keys=['sentence', 'label'])\n    \n    train_ds = Dataset.from_pandas(train_df)\n    test_ds = Dataset.from_pandas(test_df)\n    \n    return train_ds, test_ds","metadata":{"_uuid":"7d9a8157-0f6b-4885-b736-4194c89e0997","_cell_guid":"79b1038a-056a-4bb9-acf7-4d5328f3f5d1","collapsed":false,"execution":{"iopub.status.busy":"2023-04-21T18:35:09.766925Z","iopub.execute_input":"2023-04-21T18:35:09.767691Z","iopub.status.idle":"2023-04-21T18:35:09.780692Z","shell.execute_reply.started":"2023-04-21T18:35:09.767652Z","shell.execute_reply":"2023-04-21T18:35:09.779232Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model(data, checkpoint, epochs):\n    train_ds, test_ds = load_data(data, checkpoint)\n    \n    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n    \n    def t5_pre(examples):\n        model_inputs = tokenizer(examples['sentence'], max_length=128, truncation=True)\n\n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(examples['label'], max_length=128, truncation=True)\n\n        model_inputs['labels'] = labels['input_ids']\n        model_inputs['decoder_input_ids'] = np.zeros((len(labels['input_ids']), 0))\n\n        return model_inputs\n\n    train_inp = train_ds.map(t5_pre, batched=True)\n    train_inp = train_inp.remove_columns(['sentence', 'label'])\n    \n    model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\n\n    batch_size = 8\n    \n    tf_train = train_inp.to_tf_dataset(\n      columns=[\"attention_mask\", \"input_ids\", 'decoder_input_ids', 'labels'],\n      shuffle=True,\n      collate_fn=data_collator,\n      batch_size=batch_size,\n    )\n    \n    num_train_steps = len(tf_train) * epochs\n    \n    optimizer, schedule = create_optimizer(\n        init_lr=3e-4,\n        num_warmup_steps=0,\n        num_train_steps=num_train_steps,\n        weight_decay_rate=0.01,\n    )\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=tf.metrics.SparseCategoricalAccuracy(),\n    )\n    \n    model.fit(\n      tf_train,\n      epochs=epochs,\n      batch_size=batch_size\n    )\n    \n    test_inp = test_ds.map(t5_pre, batched=True)\n    test_inp = test_inp.remove_columns(['sentence', 'label'])\n    \n    tf_test = test_inp.to_tf_dataset(\n        columns=[\"attention_mask\", \"input_ids\", 'decoder_input_ids', 'labels'],\n        collate_fn=data_collator,\n        shuffle=False,\n        batch_size=batch_size,\n    )\n    \n    model.evaluate(tf_test)\n    return model","metadata":{"_uuid":"dd4f3092-069c-42f2-a581-e18cc2ad0342","_cell_guid":"93a5cae0-3684-4bda-b453-1593bdc4a439","collapsed":false,"id":"_OoF9mebuSZc","outputId":"e53c9ecb-7b19-45d0-bea4-a1cf05d1f32b","execution":{"iopub.status.busy":"2023-04-21T19:45:04.701728Z","iopub.execute_input":"2023-04-21T19:45:04.706202Z","iopub.status.idle":"2023-04-21T19:45:04.721071Z","shell.execute_reply.started":"2023-04-21T19:45:04.706159Z","shell.execute_reply":"2023-04-21T19:45:04.720009Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_model(data_name, model_checkpoint (on huggingface), #epochs)\nsst2_t5_model = test_model(\"sst2\", \"t5-small\", 3)","metadata":{"_uuid":"3af9407a-770d-4546-9f56-a59cf5d610ed","_cell_guid":"d2f3517b-95db-425b-83c0-f1ebe68f8059","collapsed":false,"execution":{"iopub.status.busy":"2023-04-21T19:47:44.549201Z","iopub.execute_input":"2023-04-21T19:47:44.550209Z","iopub.status.idle":"2023-04-21T19:53:03.196005Z","shell.execute_reply.started":"2023-04-21T19:47:44.550157Z","shell.execute_reply":"2023-04-21T19:53:03.194873Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sst5_t5_model = test_model(\"sst5\", \"t5-small\", 3)","metadata":{"_uuid":"79bbd281-a858-46fa-81e0-82e7ee756fd8","_cell_guid":"5b4f4418-0d47-4034-a094-d18e36d3dae2","collapsed":false,"execution":{"iopub.status.busy":"2023-04-21T19:46:02.694713Z","iopub.execute_input":"2023-04-21T19:46:02.695121Z","iopub.status.idle":"2023-04-21T19:47:11.365652Z","shell.execute_reply.started":"2023-04-21T19:46:02.695084Z","shell.execute_reply":"2023-04-21T19:47:11.364067Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sst2_t5_model = test_model(\"fin\", \"t5-small\", 3)","metadata":{"_uuid":"ca861da1-a42f-4416-b67e-ecd77e040976","_cell_guid":"f06101b2-7e98-48e6-80c9-d98d85a4d296","collapsed":false,"execution":{"iopub.status.busy":"2023-04-21T19:09:20.735951Z","iopub.execute_input":"2023-04-21T19:09:20.737151Z","iopub.status.idle":"2023-04-21T19:12:12.220484Z","shell.execute_reply.started":"2023-04-21T19:09:20.737090Z","shell.execute_reply":"2023-04-21T19:12:12.219339Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_t5_model = test_model(\"imdb\", \"t5-small\", 5)","metadata":{"_uuid":"6ca0a602-3a03-4574-bc1c-b8f785cc8876","_cell_guid":"6df2e21a-dc07-4a71-bed9-14a6e88f6286","collapsed":false,"execution":{"iopub.status.busy":"2023-04-21T19:34:09.269492Z","iopub.execute_input":"2023-04-21T19:34:09.270503Z","iopub.status.idle":"2023-04-21T19:43:07.083714Z","shell.execute_reply.started":"2023-04-21T19:34:09.270452Z","shell.execute_reply":"2023-04-21T19:43:07.082211Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"2d4509b4-ad78-4bb0-bba1-0f2ff171c02e","_cell_guid":"80a27ef8-06c7-46aa-9742-9fa9b2442a8f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}